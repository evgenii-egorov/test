---
title: "Implicit Metropolis-Hastings"
collection: talks
type: "Talk"
permalink: /talks/2012-03-01-talk-1
venue: "HSE, Computer Science Faculty"
date: 2020-01-20
location: "Moscow, Russia"
---

We have the meeting between "Machine Learning"-people and Physicists at the wonderful workshop [Physics inspired Machine Learning](https://cs.hse.ru/en/big-data/bayeslab/announcements/332994498.html). I presented our (Kirill Neklyudov, Evgenii Egorov, Dmitry Vetrov) [paper](https://arxiv.org/abs/1906.03644) on the fusion between MCMC and GANs. There is a [video](https://youtu.be/i-k2SFmhuAw?list=PLEqoHzpnmTfCwxT3H2-7MtN3Po60-a2fY) (in Russian) and very long [slides](imh/IMH_presentation.pdf). If you would like to take the global idea instantly, I propose to see our NeurIPS [poster](imh/IMH_NeurIPS2019_poster.pdf).

Abstract of the paper:

Recent works propose using the discriminator of a GAN to filter out unrealistic samples of the generator. We generalize these ideas by introducing the implicit Metropolis-Hastings algorithm. For any implicit probabilistic model and a target distribution represented by a set of samples, implicit Metropolis-Hastings operates by learning a discriminator to estimate the density-ratio and then generating a chain of samples. Since the approximation of density ratio introduces an error on every step of the chain, it is crucial to analyze the stationary distribution of such chain. For that purpose, we present a theoretical result stating that the discriminator loss upper bounds the total variation distance between the target distribution and the stationary distribution. Finally, we validate the proposed algorithm both for independent and Markov proposals on CIFAR-10 and CelebA datasets.
